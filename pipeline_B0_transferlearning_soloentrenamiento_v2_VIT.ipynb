{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "##import libraries\n",
    "from tinyimagenet import TinyImageNet\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.utils.data as data\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import poutyne\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import sys\n",
    "import random\n",
    "\n",
    "#/Users/tatibada/Documents/Tesis Maestria DM/scripts/EfficientNet/pipeline_B0_weightsnone_soloentrenamiento_v2.py\n",
    "#sys.path.append('/content/drive/MyDrive/Tesis de Ms Data Mining TBadaracco/Notebooks/')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import transformations\n"
     ]
    }
   ],
   "source": [
    "# load transformations\n",
    "import transformaciones as tr\n",
    "\n",
    "rotation_transforms = tr.rotation_transforms()\n",
    "translation_transforms = tr.translation_transforms()\n",
    "scale_transforms = tr.scale_transforms()\n",
    "perspective_transforms = tr.perspective_transforms()\n",
    "brightness_transforms = [tr.brightness_transforms(factor) for factor in tr.brightness_parameters]\n",
    "contrast_transformations = [tr.contrast_transforms(alpha) for alpha in tr.contrast_list]\n",
    "grayscale_transformations = [tr.grayscale_transforms(alpha) for alpha in tr.grey_list]\n",
    "solarize_transformations = [tr.solarize_transforms(threshold) for threshold in tr.solarization_thresholds]\n",
    "posterize_transformations = [tr.posterize_transforms(alpha) for alpha in tr.posterize_list]\n",
    "invertion_transformations = [tr.invertion_transforms(alpha) for alpha in tr.invertion_list]\n",
    "\n",
    "\n",
    "transformation_afin = [rotation_transforms,\n",
    "                       translation_transforms,\n",
    "                       scale_transforms,\n",
    "                       perspective_transforms,\n",
    "                       brightness_transforms,\n",
    "                       contrast_transformations,\n",
    "                       grayscale_transformations,\n",
    "                       solarize_transformations,\n",
    "                       posterize_transformations,\n",
    "                       invertion_transformations]\n",
    "\n",
    "\n",
    "transformaciones = ['rotacion','traslacion','escala','proyeccion','brillo','contraste','escala_grises','solarizacion','posterizacion','inversion_colores']\n",
    "\n",
    "print('import transformations')\n",
    "\n",
    "#### para salucionar error: RuntimeError: invalid hash value (expected \"7eb33cd5\", got \"23ab8bcd5bdbef61a7a43b91adcad81f622fd7f36fb4935a569828d77888c44e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/fastervit/models/faster_vit.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from timm.models import _builder\n",
    "\n",
    "_builder._update_default_kwargs = _builder._update_default_model_kwargs\n",
    "\n",
    "## fastervit\n",
    "from fastervit import create_model\n",
    "# Define fastervit-0 model with 224 x 224 resolution\n",
    "\n",
    "base_model = create_model('faster_vit_0_224', \n",
    "                          pretrained=True,\n",
    "                          model_path=\"/home/tbadaracco//faster_vit_0.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterViTLayer(\n",
       "  (blocks): ModuleList(\n",
       "    (0): HAT(\n",
       "      (pos_embed): PosEmbMLPSwinv1D(\n",
       "        (cpb_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): WindowAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.147)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): HAT(\n",
       "      (pos_embed): PosEmbMLPSwinv1D(\n",
       "        (cpb_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): WindowAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.160)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): HAT(\n",
       "      (pos_embed): PosEmbMLPSwinv1D(\n",
       "        (cpb_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): WindowAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.173)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): HAT(\n",
       "      (pos_embed): PosEmbMLPSwinv1D(\n",
       "        (cpb_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): WindowAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.187)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): HAT(\n",
       "      (pos_embed): PosEmbMLPSwinv1D(\n",
       "        (cpb_mlp): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): WindowAttention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (drop_path): DropPath(drop_prob=0.200)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.levels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar los parámetros del último bloque de \"features\" (bloque 8)\n",
    "for param in base_model.levels[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Descongelar los parámetros de la capa de clasificación\n",
    "for param in base_model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "tinyimagenet_classes = 200\n",
    "# Reemplazar el clasificador del modelo FasterViT\n",
    "base_model.head = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True),\n",
    "    torch.nn.Linear(512, tinyimagenet_classes),  # Ajustar el tamaño según la salida de la capa anterior\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with transformation: Without_transformation\n",
      "checkpoint path: models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "checkpoint optimizer path: models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "DataLoader listo\n",
      "Epoch:  1/20 Train steps: 3125 Val steps: 313 8m7.92s loss: 1.804278 acc: 57.500000 top5: 81.248000 fscore_macro: 0.573246 val_loss: 1.267904 val_acc: 68.360000 val_top5: 89.090000 val_fscore_macro: 0.683960\n",
      "Epoch 1: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 1: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  2/20 Train steps: 3125 Val steps: 313 8m8.53s loss: 1.291513 acc: 67.405000 top5: 88.217000 fscore_macro: 0.673325 val_loss: 1.263132 val_acc: 68.420000 val_top5: 88.550000 val_fscore_macro: 0.684463\n",
      "Epoch 2: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 2: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  3/20 Train steps: 3125 Val steps: 313 8m10.35s loss: 1.103188 acc: 71.607000 top5: 90.771000 fscore_macro: 0.715582 val_loss: 1.232691 val_acc: 69.240000 val_top5: 89.080000 val_fscore_macro: 0.693895\n",
      "Epoch 3: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 3: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  4/20 Train steps: 3125 Val steps: 313 8m9.05s loss: 0.966775 acc: 74.735000 top5: 92.437000 fscore_macro: 0.746969 val_loss: 1.269827 val_acc: 69.580000 val_top5: 89.150000 val_fscore_macro: 0.696126\n",
      "Epoch 4: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 4: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  5/20 Train steps: 3125 Val steps: 313 8m9.94s loss: 0.865824 acc: 76.971000 top5: 93.624000 fscore_macro: 0.769457 val_loss: 1.317729 val_acc: 69.100000 val_top5: 88.510000 val_fscore_macro: 0.691131\n",
      "Epoch 5: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 5: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  6/20 Train steps: 3125 Val steps: 313 8m9.69s loss: 0.782803 acc: 78.835000 top5: 94.475000 fscore_macro: 0.788146 val_loss: 1.368592 val_acc: 69.510000 val_top5: 88.680000 val_fscore_macro: 0.694813\n",
      "Epoch 6: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 6: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  7/20 Train steps: 3125 Val steps: 313 8m11.88s loss: 0.718071 acc: 80.338000 top5: 95.334000 fscore_macro: 0.803213 val_loss: 1.371816 val_acc: 69.300000 val_top5: 88.460000 val_fscore_macro: 0.692064\n",
      "Epoch 7: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 7: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  8/20 Train steps: 3125 Val steps: 313 8m15.34s loss: 0.664054 acc: 81.782000 top5: 95.903000 fscore_macro: 0.817647 val_loss: 1.438892 val_acc: 69.480000 val_top5: 88.170000 val_fscore_macro: 0.694417\n",
      "Epoch 8: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 8: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  9/20 Train steps: 3125 Val steps: 313 8m17.96s loss: 0.621745 acc: 82.744000 top5: 96.381000 fscore_macro: 0.827348 val_loss: 1.458823 val_acc: 69.280000 val_top5: 88.260000 val_fscore_macro: 0.692821\n",
      "Epoch 9: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 9: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 10/20 Train steps: 3125 Val steps: 313 8m17.62s loss: 0.587062 acc: 83.588000 top5: 96.676000 fscore_macro: 0.835823 val_loss: 1.487987 val_acc: 68.820000 val_top5: 88.320000 val_fscore_macro: 0.688367\n",
      "Epoch 10: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 10: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 11/20 Train steps: 3125 Val steps: 313 8m17.33s loss: 0.545584 acc: 84.705000 top5: 97.040000 fscore_macro: 0.846986 val_loss: 1.483332 val_acc: 69.430000 val_top5: 88.330000 val_fscore_macro: 0.694334\n",
      "Epoch 11: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 11: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 12/20 Train steps: 3125 Val steps: 313 8m17.12s loss: 0.520202 acc: 85.382000 top5: 97.279000 fscore_macro: 0.853749 val_loss: 1.553787 val_acc: 68.760000 val_top5: 88.350000 val_fscore_macro: 0.688975\n",
      "Epoch 12: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 12: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 13/20 Train steps: 3125 Val steps: 313 8m17.43s loss: 0.497695 acc: 85.900000 top5: 97.471000 fscore_macro: 0.858982 val_loss: 1.561407 val_acc: 68.680000 val_top5: 88.290000 val_fscore_macro: 0.687678\n",
      "Epoch 13: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 13: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 14/20 Train steps: 3125 Val steps: 313 8m17.78s loss: 0.478087 acc: 86.579000 top5: 97.600000 fscore_macro: 0.865768 val_loss: 1.612029 val_acc: 68.770000 val_top5: 87.730000 val_fscore_macro: 0.687617\n",
      "Epoch 14: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 14: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 15/20 Train steps: 3125 Val steps: 313 8m16.04s loss: 0.460675 acc: 86.928000 top5: 97.757000 fscore_macro: 0.869262 val_loss: 1.564406 val_acc: 68.590000 val_top5: 88.070000 val_fscore_macro: 0.684685\n",
      "Epoch 15: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 15: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 16/20 Train steps: 3125 Val steps: 313 8m17.03s loss: 0.446153 acc: 87.450000 top5: 97.823000 fscore_macro: 0.874471 val_loss: 1.649627 val_acc: 68.160000 val_top5: 88.050000 val_fscore_macro: 0.681726\n",
      "Epoch 16: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 16: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 17/20 Train steps: 3125 Val steps: 313 8m15.99s loss: 0.427377 acc: 87.813000 top5: 98.028000 fscore_macro: 0.878115 val_loss: 1.607779 val_acc: 67.980000 val_top5: 87.900000 val_fscore_macro: 0.679954\n",
      "Epoch 17: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 17: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 18/20 Train steps: 3125 Val steps: 313 8m15.48s loss: 0.416578 acc: 88.195000 top5: 98.136000 fscore_macro: 0.881945 val_loss: 1.697203 val_acc: 68.450000 val_top5: 88.090000 val_fscore_macro: 0.683961\n",
      "Epoch 18: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 18: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 19/20 Train steps: 3125 Val steps: 313 8m17.23s loss: 0.396604 acc: 88.661000 top5: 98.237000 fscore_macro: 0.886582 val_loss: 1.614484 val_acc: 68.700000 val_top5: 87.770000 val_fscore_macro: 0.687892\n",
      "Epoch 19: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 19: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 20/20 Train steps: 3125 Val steps: 313 8m16.16s loss: 0.388347 acc: 88.926000 top5: 98.346000 fscore_macro: 0.889251 val_loss: 1.653811 val_acc: 68.920000 val_top5: 87.660000 val_fscore_macro: 0.689234\n",
      "Epoch 20: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 20: saving file to models/ViT/transfer_learning/Without_transformation/checkpoint_opt.ckpt\n"
     ]
    }
   ],
   "source": [
    "parameters_to_optimize = [\n",
    "   \n",
    "    {'params': base_model.head.parameters(), 'lr': 0.001},\n",
    "    \n",
    "    {'params': base_model.levels[-1].parameters(), 'lr': 0.0005} \n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(parameters_to_optimize)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Directorio principal donde se guardarán los resultados\n",
    "main_dir = 'models/ViT/transfer_learning'\n",
    "\n",
    "# Crear directorio principal si no existe\n",
    "Path(main_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "class TinyImageNet(TinyImageNet):\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        return x,y\n",
    "transformaciones = ['Without_transformation']\n",
    "# Iterar sobre cada transformación\n",
    "for i, transformacion in enumerate(transformaciones):\n",
    "    print(f\"Training with transformation: {transformacion}\")\n",
    "\n",
    "    # Crear directorio para la transformación actual\n",
    "    transform_dir = os.path.join(main_dir, transformacion)\n",
    "    Path(transform_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Definir el path del checkpoint\n",
    "    checkpoint_path = os.path.join(transform_dir, 'checkpoint_last.ckpt')\n",
    "    checkpoint_opt_path = os.path.join(transform_dir, 'checkpoint_opt.ckpt') \n",
    "\n",
    "    print(f'checkpoint path: {checkpoint_path}')\n",
    "    print(f'checkpoint optimizer path: {checkpoint_opt_path}')\n",
    "\n",
    "    # Cargar o reiniciar el modelo y el checkpoint para cada transformación\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # Carga el estado del optimizador desde el archivo\n",
    "    if os.path.exists(checkpoint_opt_path):\n",
    "        optimizer.load_state_dict(torch.load(checkpoint_opt_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # Definir el conjunto de datos con la transformación actual\n",
    "    current_transform = transformation_afin[i]\n",
    "    #print(current_transform)\n",
    "\n",
    "    random_ts = lambda x: random.choice(current_transform)(x)\n",
    "    \n",
    "    normalize_transform = T.Compose(\n",
    "        [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.transforms.Normalize(TinyImageNet.mean,TinyImageNet.std),\n",
    "        #random_ts\n",
    "        ])\n",
    "\n",
    "    # Definir el conjunto de datos original\n",
    "    dataset_train = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"), split=\"train\", imagenet_idx=False, transform=normalize_transform)\n",
    "    dataset_val = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"), split=\"val\", imagenet_idx=False, transform=normalize_transform)\n",
    "\n",
    "    #print(dataset_train[0])\n",
    "    # Definir un DataLoader para cargar los datos originales por lotes\n",
    "    train_loader = data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "    val_loader = data.DataLoader(dataset_val, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "    print('DataLoader listo')\n",
    "\n",
    "\n",
    "    # Definir el trainer\n",
    "    trainer = poutyne.Model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        'cross_entropy',\n",
    "        batch_metrics=['accuracy', poutyne.TopKAccuracy(5)],\n",
    "        epoch_metrics=['f1'],\n",
    "        device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    # Definir el historial y el checkpoint para cada transformación\n",
    "    history_path = os.path.join(transform_dir, 'history.csv')\n",
    "    checkpoint_path = os.path.join(transform_dir, 'checkpoint_last.ckpt')\n",
    "    best_checkpoint_path = os.path.join(transform_dir, 'tmp_best.ckpt')\n",
    "    checkpoint_opt_path = os.path.join(transform_dir, 'checkpoint_opt.ckpt')   \n",
    "\n",
    "    checkpoint = poutyne.ModelCheckpoint(checkpoint_path, monitor='val_acc', mode='max', save_best_only=False, restore_best=False, verbose=True, temporary_filename=best_checkpoint_path)\n",
    "    opt_checkpoint = poutyne.OptimizerCheckpoint(checkpoint_opt_path, monitor='val_acc', mode='max', save_best_only=False, restore_best=False, verbose=True,        temporary_filename=best_checkpoint_path)\n",
    "\n",
    "    class HistorySaver(poutyne.Callback):\n",
    "\n",
    "      def __init__(self,filepath):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.history = []\n",
    "\n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "        self.history.append(logs)\n",
    "\n",
    "        if os.path.exists(history_path):\n",
    "          df1 = pd.read_csv(history_path)\n",
    "          df = pd.DataFrame(self.history)\n",
    "          df = pd.concat([df1,df])\n",
    "          df.reset_index(drop=True,inplace=True)\n",
    "          df.drop_duplicates(inplace = True, ignore_index = True)\n",
    "          df.to_csv(self.filepath,index=False)\n",
    "        else:\n",
    "          df = pd.DataFrame(self.history)\n",
    "          df.to_csv(self.filepath,index=False)\n",
    "\n",
    "    # callback personalizado\n",
    "    history_saver = HistorySaver(history_path)\n",
    "\n",
    "    if os.path.exists(history_path):\n",
    "        df1 = pd.read_csv(history_path)\n",
    "        history_saver.history = df1.to_dict('records')\n",
    "\n",
    "    # Entrenar el modelo para cada transformación\n",
    "    #history = trainer.fit_dataset(train_loader, valid_dataset=val_loader, batch_size=8, epochs=num_epochs, callbacks=[checkpoint, opt_checkpoint,history_saver])\n",
    "\n",
    "    history = trainer.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=[checkpoint, opt_checkpoint, history_saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
