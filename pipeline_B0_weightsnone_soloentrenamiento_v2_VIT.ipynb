{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "##import libraries\n",
    "from tinyimagenet import TinyImageNet\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.utils.data as data\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import poutyne\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import sys\n",
    "import random\n",
    "\n",
    "#/Users/tatibada/Documents/Tesis Maestria DM/scripts/EfficientNet/pipeline_B0_weightsnone_soloentrenamiento_v2.py\n",
    "#sys.path.append('/content/drive/MyDrive/Tesis de Ms Data Mining TBadaracco/Notebooks/')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import transformations\n"
     ]
    }
   ],
   "source": [
    "# load transformations\n",
    "import transformaciones as tr\n",
    "\n",
    "rotation_transforms = tr.rotation_transforms()\n",
    "translation_transforms = tr.translation_transforms()\n",
    "scale_transforms = tr.scale_transforms()\n",
    "perspective_transforms = tr.perspective_transforms()\n",
    "brightness_transforms = [tr.brightness_transforms(factor) for factor in tr.brightness_parameters]\n",
    "contrast_transformations = [tr.contrast_transforms(alpha) for alpha in tr.contrast_list]\n",
    "grayscale_transformations = [tr.grayscale_transforms(alpha) for alpha in tr.grey_list]\n",
    "solarize_transformations = [tr.solarize_transforms(threshold) for threshold in tr.solarization_thresholds]\n",
    "posterize_transformations = [tr.posterize_transforms(alpha) for alpha in tr.posterize_list]\n",
    "invertion_transformations = [tr.invertion_transforms(alpha) for alpha in tr.invertion_list]\n",
    "\n",
    "\n",
    "transformation_afin = [rotation_transforms,\n",
    "                       translation_transforms,\n",
    "                       scale_transforms,\n",
    "                       perspective_transforms,\n",
    "                       brightness_transforms,\n",
    "                       contrast_transformations,\n",
    "                       grayscale_transformations,\n",
    "                       solarize_transformations,\n",
    "                       posterize_transformations,\n",
    "                       invertion_transformations]\n",
    "\n",
    "\n",
    "transformaciones = ['rotacion','traslacion','escala','proyeccion','brillo','contraste','escala_grises','solarizacion','posterizacion','inversion_colores']\n",
    "\n",
    "print('import transformations')\n",
    "\n",
    "#### para salucionar error: RuntimeError: invalid hash value (expected \"7eb33cd5\", got \"23ab8bcd5bdbef61a7a43b91adcad81f622fd7f36fb4935a569828d77888c44e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/tbadaracco/tb_env/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from timm.models import _builder\n",
    "\n",
    "_builder._update_default_kwargs = _builder._update_default_model_kwargs\n",
    "\n",
    "## fastervit\n",
    "from fastervit import create_model\n",
    "# Define fastervit-0 model with 224 x 224 resolution\n",
    "\n",
    "base_model = create_model('faster_vit_0_224', \n",
    "                          pretrained=False,\n",
    "                          model_path=\"/home/tbadaracco//faster_vit_0.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Identity()\n",
       "    (conv_down): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (levels): ModuleList(\n",
       "    (0): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.040)\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.053)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.067)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.080)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.080)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.093)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.093)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.107)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.107)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.120)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.120)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (5): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.133)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.133)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (global_tokenizer): TokenInitializer(\n",
       "        (pos_embed): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "        (to_global_feature): Sequential(\n",
       "          (pos): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.147)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.160)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.173)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.187)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (head): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with transformation: Without_transformation\n",
      "checkpoint path: models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "checkpoint optimizer path: models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "DataLoader listo\n",
      "Epoch:  1/20 Train steps: 12500 Val steps: 1250 22m33.52s loss: 4.866313 acc: 3.926000 top5: 13.654000 fscore_macro: 0.028458 val_loss: 4.374272 val_acc: 8.880000 val_top5: 25.610000 val_fscore_macro: 0.061971\n",
      "Epoch 1: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 1: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  2/20 Train steps: 12500 Val steps: 1250 22m37.03s loss: 4.350873 acc: 9.917000 top5: 26.745000 fscore_macro: 0.080693 val_loss: 3.960419 val_acc: 14.620000 val_top5: 36.220000 val_fscore_macro: 0.120816\n",
      "Epoch 2: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 2: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  3/20 Train steps: 12500 Val steps: 1250 22m38.91s loss: 4.005643 acc: 14.735000 top5: 35.287000 fscore_macro: 0.127673 val_loss: 3.638533 val_acc: 19.480000 val_top5: 43.550000 val_fscore_macro: 0.167914\n",
      "Epoch 3: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 3: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  4/20 Train steps: 12500 Val steps: 1250 22m37.03s loss: 3.771652 acc: 18.079000 top5: 40.722000 fscore_macro: 0.161051 val_loss: 3.461837 val_acc: 22.320000 val_top5: 47.490000 val_fscore_macro: 0.196269\n",
      "Epoch 4: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 4: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  5/20 Train steps: 12500 Val steps: 1250 22m37.45s loss: 3.614555 acc: 20.695000 top5: 44.321000 fscore_macro: 0.187534 val_loss: 3.342027 val_acc: 24.810000 val_top5: 50.160000 val_fscore_macro: 0.220615\n",
      "Epoch 5: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 5: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  6/20 Train steps: 12500 Val steps: 1250 22m36.04s loss: 3.484461 acc: 22.767000 top5: 47.254000 fscore_macro: 0.209376 val_loss: 3.235933 val_acc: 26.600000 val_top5: 52.340000 val_fscore_macro: 0.244191\n",
      "Epoch 6: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 6: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  7/20 Train steps: 12500 Val steps: 1250 22m35.45s loss: 3.365014 acc: 24.976000 top5: 49.836000 fscore_macro: 0.232229 val_loss: 3.107073 val_acc: 28.840000 val_top5: 55.060000 val_fscore_macro: 0.264829\n",
      "Epoch 7: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 7: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  8/20 Train steps: 12500 Val steps: 1250 22m32.06s loss: 3.263086 acc: 26.649000 top5: 52.152000 fscore_macro: 0.249768 val_loss: 3.078625 val_acc: 29.690000 val_top5: 56.200000 val_fscore_macro: 0.275078\n",
      "Epoch 8: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 8: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch:  9/20 Train steps: 12500 Val steps: 1250 22m43.54s loss: 3.166407 acc: 28.395000 top5: 54.065000 fscore_macro: 0.268300 val_loss: 3.029570 val_acc: 30.420000 val_top5: 57.200000 val_fscore_macro: 0.283573\n",
      "Epoch 9: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 9: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 10/20 Train steps: 12500 Val steps: 1250 22m33.84s loss: 3.081628 acc: 29.777000 top5: 56.048000 fscore_macro: 0.283048 val_loss: 2.936220 val_acc: 32.190000 val_top5: 59.150000 val_fscore_macro: 0.301293\n",
      "Epoch 10: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 10: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 11/20 Train steps: 12500 Val steps: 1250 22m34.69s loss: 2.992417 acc: 31.371000 top5: 57.918000 fscore_macro: 0.299520 val_loss: 2.861229 val_acc: 33.470000 val_top5: 61.130000 val_fscore_macro: 0.313093\n",
      "Epoch 11: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 11: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 12/20 Train steps: 12500 Val steps: 1250 22m34.84s loss: 2.908050 acc: 33.121000 top5: 59.600000 fscore_macro: 0.318040 val_loss: 2.821204 val_acc: 34.280000 val_top5: 61.790000 val_fscore_macro: 0.328414\n",
      "Epoch 12: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 12: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 13/20 Train steps: 12500 Val steps: 1250 22m33.00s loss: 2.826138 acc: 34.377000 top5: 61.330000 fscore_macro: 0.331263 val_loss: 2.747109 val_acc: 36.250000 val_top5: 63.400000 val_fscore_macro: 0.345191\n",
      "Epoch 13: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 13: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 14/20 Train steps: 12500 Val steps: 1250 22m33.04s loss: 2.745439 acc: 36.045000 top5: 62.917000 fscore_macro: 0.348459 val_loss: 2.683924 val_acc: 37.220000 val_top5: 64.610000 val_fscore_macro: 0.360670\n",
      "Epoch 14: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 14: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 15/20 Train steps: 12500 Val steps: 1250 22m33.67s loss: 2.670409 acc: 37.421000 top5: 64.551000 fscore_macro: 0.363603 val_loss: 2.670313 val_acc: 37.920000 val_top5: 64.840000 val_fscore_macro: 0.366393\n",
      "Epoch 15: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 15: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 16/20 Train steps: 12500 Val steps: 1250 22m31.01s loss: 2.598837 acc: 38.606000 top5: 66.011000 fscore_macro: 0.375892 val_loss: 2.607130 val_acc: 39.430000 val_top5: 66.220000 val_fscore_macro: 0.379786\n",
      "Epoch 16: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 16: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 17/20 Train steps: 12500 Val steps: 1250 22m32.34s loss: 2.525269 acc: 40.013000 top5: 67.261000 fscore_macro: 0.390207 val_loss: 2.583660 val_acc: 39.820000 val_top5: 67.030000 val_fscore_macro: 0.383615\n",
      "Epoch 17: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 17: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 18/20 Train steps: 12500 Val steps: 1250 22m31.74s loss: 2.448077 acc: 41.571000 top5: 68.785000 fscore_macro: 0.406500 val_loss: 2.545754 val_acc: 40.700000 val_top5: 67.840000 val_fscore_macro: 0.394680\n",
      "Epoch 18: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 18: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 19/20 Train steps: 12500 Val steps: 1250 22m35.75s loss: 2.382149 acc: 42.984000 top5: 70.046000 fscore_macro: 0.421235 val_loss: 2.524188 val_acc: 41.760000 val_top5: 68.460000 val_fscore_macro: 0.405059\n",
      "Epoch 19: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 19: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n",
      "Epoch: 20/20 Train steps: 12500 Val steps: 1250 22m32.70s loss: 2.307668 acc: 44.448000 top5: 71.454000 fscore_macro: 0.436307 val_loss: 2.473324 val_acc: 42.900000 val_top5: 69.170000 val_fscore_macro: 0.414888\n",
      "Epoch 20: saving file to models/ViT/weights_none/Without_transformation/checkpoint_last.ckpt\n",
      "Epoch 20: saving file to models/ViT/weights_none/Without_transformation/checkpoint_opt.ckpt\n"
     ]
    }
   ],
   "source": [
    "# def get_state_dict(self, *args, **kwargs):\n",
    "#     kwargs.pop(\"check_hash\")\n",
    "#     return load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "\n",
    "# WeightsEnum.get_state_dict = get_state_dict\n",
    "# #####\n",
    "\n",
    "# Definir el modelo y checkpoint\n",
    "# weights = None #models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "# base_model = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "tinyimagenet_classes = 200\n",
    "# base_model.classifier = torch.nn.Sequential(\n",
    "#     torch.nn.Dropout(p=0.2, inplace=True),\n",
    "#     torch.nn.Linear(1280, tinyimagenet_classes),\n",
    "# )\n",
    "base_model.head = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True),\n",
    "    torch.nn.Linear(512, tinyimagenet_classes),  # Ajustar el tamaño según la salida de la capa anterior\n",
    ")\n",
    "#model = torch.nn.Sequential(\n",
    "   #T.Normalize(TinyImageNet.mean,TinyImageNet.std),\n",
    "    #weights.transforms(),\n",
    " #   base_model,\n",
    "#)\n",
    "\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Directorio principal donde se guardarán los resultados\n",
    "main_dir = 'models/ViT/weights_none'\n",
    "\n",
    "# Crear directorio principal si no existe\n",
    "Path(main_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "class TinyImageNet(TinyImageNet):\n",
    "    def __getitem__(self, index):\n",
    "        x, y = super().__getitem__(index)\n",
    "        return x,y\n",
    "\n",
    "# Iterar sobre cada transformación\n",
    "#transformaciones = ['Without_transformation']\n",
    "for i, transformacion in enumerate(transformaciones):\n",
    "    print(f\"Training with transformation: {transformacion}\")\n",
    "\n",
    "    # Crear directorio para la transformación actual\n",
    "    transform_dir = os.path.join(main_dir, transformacion)\n",
    "    Path(transform_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Definir el path del checkpoint\n",
    "    checkpoint_path = os.path.join(transform_dir, 'checkpoint_last.ckpt')\n",
    "    checkpoint_opt_path = os.path.join(transform_dir, 'checkpoint_opt.ckpt') \n",
    "\n",
    "    print(f'checkpoint path: {checkpoint_path}')\n",
    "    print(f'checkpoint optimizer path: {checkpoint_opt_path}')\n",
    "\n",
    "    # Cargar o reiniciar el modelo y el checkpoint para cada transformación\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # Carga el estado del optimizador desde el archivo\n",
    "    if os.path.exists(checkpoint_opt_path):\n",
    "        optimizer.load_state_dict(torch.load(checkpoint_opt_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # Definir el conjunto de datos con la transformación actual\n",
    "    current_transform = transformation_afin[i]\n",
    "    #print(current_transform)\n",
    "\n",
    "    random_ts = lambda x: random.choice(current_transform)(x)\n",
    "    \n",
    "    normalize_transform = T.Compose(\n",
    "        [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.transforms.Normalize(TinyImageNet.mean,TinyImageNet.std),\n",
    "        random_ts\n",
    "        ])\n",
    "\n",
    "    # Definir el conjunto de datos original\n",
    "    dataset_train = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"), split=\"train\", imagenet_idx=False, transform=normalize_transform)\n",
    "    dataset_val = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\"), split=\"val\", imagenet_idx=False, transform=normalize_transform)\n",
    "\n",
    "    #print(dataset_train[0])\n",
    "    # Definir un DataLoader para cargar los datos originales por lotes\n",
    "    train_loader = data.DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "    val_loader = data.DataLoader(dataset_val, batch_size=8, shuffle=True)\n",
    "\n",
    "    #for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    #    print(inputs.shape)  # Imprime la forma de los datos de entrada\n",
    "    #    print(targets.shape)  # Imprime la forma de las etiquetas\n",
    "    #    break  # Solo imprime el primer lote para evitar demasiada salida\n",
    "\n",
    "    print('DataLoader listo')\n",
    "\n",
    "\n",
    "    # Definir el trainer\n",
    "    trainer = poutyne.Model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        'cross_entropy',\n",
    "        batch_metrics=['accuracy', poutyne.TopKAccuracy(5)],\n",
    "        epoch_metrics=['f1'],\n",
    "        device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    # Definir el historial y el checkpoint para cada transformación\n",
    "    history_path = os.path.join(transform_dir, 'history.csv')\n",
    "    checkpoint_path = os.path.join(transform_dir, 'checkpoint_last.ckpt')\n",
    "    best_checkpoint_path = os.path.join(transform_dir, 'tmp_best.ckpt')\n",
    "    checkpoint_opt_path = os.path.join(transform_dir, 'checkpoint_opt.ckpt')   \n",
    "\n",
    "    checkpoint = poutyne.ModelCheckpoint(checkpoint_path, monitor='val_acc', mode='max', save_best_only=False, restore_best=False, verbose=True, temporary_filename=best_checkpoint_path)\n",
    "    opt_checkpoint = poutyne.OptimizerCheckpoint(checkpoint_opt_path, monitor='val_acc', mode='max', save_best_only=False, restore_best=False, verbose=True,        temporary_filename=best_checkpoint_path)\n",
    "\n",
    "    class HistorySaver(poutyne.Callback):\n",
    "\n",
    "      def __init__(self,filepath):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.history = []\n",
    "\n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "        self.history.append(logs)\n",
    "\n",
    "        if os.path.exists(history_path):\n",
    "          df1 = pd.read_csv(history_path)\n",
    "          df = pd.DataFrame(self.history)\n",
    "          df = pd.concat([df1,df])\n",
    "          df.reset_index(drop=True,inplace=True)\n",
    "          df.drop_duplicates(inplace = True, ignore_index = True)\n",
    "          df.to_csv(self.filepath,index=False)\n",
    "        else:\n",
    "          df = pd.DataFrame(self.history)\n",
    "          df.to_csv(self.filepath,index=False)\n",
    "\n",
    "    # callback personalizado\n",
    "    history_saver = HistorySaver(history_path)\n",
    "\n",
    "    if os.path.exists(history_path):\n",
    "        df1 = pd.read_csv(history_path)\n",
    "        history_saver.history = df1.to_dict('records')\n",
    "\n",
    "    # Entrenar el modelo para cada transformación\n",
    "    #history = trainer.fit_dataset(train_loader, valid_dataset=val_loader, batch_size=8, epochs=num_epochs, callbacks=[checkpoint, opt_checkpoint,history_saver])\n",
    "\n",
    "    history = trainer.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=[checkpoint, opt_checkpoint, history_saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
